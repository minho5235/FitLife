"""
FitLife AI - KnowledgeBase (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„ íƒ‘ì¬)
"""
import os
from typing import List, Tuple
from dotenv import load_dotenv
from supabase import create_client, Client
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document

# ì„¤ì • íŒŒì¼ ë¡œë“œ
import src.config as config

load_dotenv()

class KnowledgeBase:
    def __init__(self):
        # 1. Supabase í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
        self.supabase_url = config.SUPABASE_URL
        self.supabase_key = config.SUPABASE_KEY
        
        if not self.supabase_url or not self.supabase_key:
            raise ValueError("âš ï¸ Supabase ì ‘ì† ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            
        self.supabase_client: Client = create_client(self.supabase_url, self.supabase_key)
        
        # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”Œ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... ({config.EMBEDDING_MODEL_NAME})")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=config.EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    def add_documents(self, documents: List[dict], category: str = "general"):
        """
        [ì—…ë¡œë“œìš©] ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ì—¬ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        data_to_insert = []
        
        print(f"ğŸ“¦ ë°ì´í„° ì„ë² ë”© ë³€í™˜ ì¤‘... ({len(documents)}ê°œ)")
        texts = [doc['content'] for doc in documents]
        embeddings = self.embedding_model.embed_documents(texts)
        
        for i, doc in enumerate(documents):
            data_to_insert.append({
                "content": doc['content'],
                "metadata": {
                    "title": doc['title'],
                    "source": doc.get("source", "unknown"),
                    "category": category,
                    "video_url": doc.get("video_url", ""),
                    "tags": doc.get("tags", [])  # [Update] íƒœê·¸ í•„ë“œ ì¶”ê°€
                },
                "embedding": embeddings[i]
            })
            
        try:
            self.supabase_client.table("documents").insert(data_to_insert).execute()
            print(f"âœ… {len(data_to_insert)}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    def search(self, query: str, top_k: int = 5, category: str = None) -> List[Tuple[Document, float]]:
        """
        [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„]
        ë²¡í„° ìœ ì‚¬ë„(Semantic) + í‚¤ì›Œë“œ ë§¤ì¹­(Lexical) ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬ ì¬ì •ë ¬í•©ë‹ˆë‹¤.
        """
        try:
            # 1. ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜) - ë„‰ë„‰í•˜ê²Œ 2ë°°ìˆ˜(top_k * 2)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            query_vector = self.embedding_model.embed_query(query)
            
            params = {
                "query_embedding": query_vector,
                "match_threshold": 0.1, 
                "match_count": top_k * 2 
            }
            
            response = self.supabase_client.rpc("match_documents", params).execute()
            
            # 2. íŒŒì´ì¬ ë ˆë²¨ì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ë¦¬ë­í‚¹ (Reranking)
            raw_results = []
            query_tokens = set(query.split()) # ê²€ìƒ‰ì–´ í† í°í™”
            
            for item in response.data:
                # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                meta = item.get("metadata", {})
                if category and meta.get("category") != category:
                    continue
                
                content = item.get("content", "")
                title = meta.get("title", "")
                vector_score = item['similarity']
                
                # [Hybrid Logic] í‚¤ì›Œë“œ ê°€ì‚°ì  ë¡œì§
                keyword_bonus = 0.0
                matched_count = 0
                
                for token in query_tokens:
                    if len(token) < 2: continue # 1ê¸€ìëŠ” ë¬´ì‹œ
                    if token in title:
                        keyword_bonus += 0.05 # ì œëª©ì— ìˆìœ¼ë©´ í° ê°€ì‚°ì 
                        matched_count += 1
                    elif token in content:
                        keyword_bonus += 0.02 # ë³¸ë¬¸ì— ìˆìœ¼ë©´ ì‘ì€ ê°€ì‚°ì 
                        matched_count += 1
                
                # ë„ˆë¬´ ê³¼í•œ ê°€ì‚°ì  ë°©ì§€ (ìµœëŒ€ 0.15ì  ì œí•œ)
                final_score = vector_score + min(keyword_bonus, 0.15)
                
                doc = Document(page_content=content, metadata=meta)
                raw_results.append((doc, final_score))
            
            # 3. ìµœì¢… ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            raw_results.sort(key=lambda x: x[1], reverse=True)
            
            return raw_results[:top_k]

        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def clear(self):
        """ë°ì´í„° ì´ˆê¸°í™”"""
        try:
            self.supabase_client.table("documents").delete().neq("id", "00000000-0000-0000-0000-000000000000").execute()
            print("ğŸ—‘ï¸ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì´ˆê¸°í™” ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}")